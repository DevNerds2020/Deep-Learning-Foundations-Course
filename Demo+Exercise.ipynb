{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Classifier_1, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size) # W1x + b1\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # w2h + b2\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = self.i2h(inp) # inp @ self.W_1 can be replaced by torch.matmul(inp, self.W_1)\n",
    "        h = self.relu(x)\n",
    "        y = self.h2o(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier_1(4, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have till now only been backpropagating with just one example...\n",
    "# In practice we optimize over a batch of input datapoints\n",
    "# We will now see how to deal with batches\n",
    "# Actually not much modification is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(6,4) # we have 6 data points each with 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.LongTensor([1,0,0,1,1,0]) # labels for these random outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = clf(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape # output corresponding to the 6 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2533, -0.0357],\n",
       "        [ 0.1266, -0.1031],\n",
       "        [-0.0775,  0.0793],\n",
       "        [ 0.0043, -0.0679],\n",
       "        [-0.5174,  0.2627],\n",
       "        [-0.5508,  0.3134]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(out, label) # calculated across the whole batch (6 data points) and then calculating the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7121, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training is again similar\n",
    "optimizer = optim.SGD(clf.parameters(),lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is pretty easy to extend from a single data point to a batch...\n",
    "# I often find it useful when designing a architecture to first design it for a single data point and then \n",
    "# extending it to batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Classifier_1, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size) # W1x + b1\n",
    "        self.norm = nn.LayerNorm(hidden_size) # layer norm\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # w2h + b2\n",
    "        self.relu = nn.ReLU()\n",
    "        # initilization\n",
    "        nn.init.xavier_normal_(self.i2h.weight)\n",
    "        nn.init.xavier_normal_(self.h2o.weight)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = self.i2h(inp) # inp @ self.W_1 can be replaced by torch.matmul(inp, self.W_1)\n",
    "        x = self.norm(x)\n",
    "        h = self.relu(x)\n",
    "        y = self.h2o(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier_1(4,6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.4971,  0.2784,  0.3415,  0.4611,  0.0142, -0.4708],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.i2h.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/mnist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would like to train the model in batches...\n",
    "# Ideally examples in a batch should be picked at random...\n",
    "# Of course we can code it ourselves, but pytorch provides a Datset module to do just that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/drive/folders/1y6_ddgZuxdMgHlM4BjZp9mmtwFWI7p4t?usp=sharing (Link to the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = []\n",
    "class_labels = []\n",
    "\n",
    "with open('mnist_train.csv') as fs:\n",
    "    for line in fs:\n",
    "        data = list(map(int, line.strip().split(','))) \n",
    "        label = data[0]\n",
    "        datapoint = data[1:]\n",
    "        data_points.append(datapoint)\n",
    "        class_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_dataset(Dataset):\n",
    "    def __init__(self, data_points, class_labels):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data_points\n",
    "        self.labels = class_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        # returns length of the dataset\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # retrieves an item of a given index\n",
    "        d = torch.FloatTensor(self.data[index])\n",
    "        l = torch.LongTensor([self.labels[index]])\n",
    "        return d,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = Mnist_dataset(data_points, class_labels)\n",
    "dataloader = DataLoader(mnist_data, batch_size=32, shuffle=True)\n",
    "# this will resturn a batch of 32 examples which you can directly set as input to the model clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier_1(784, 1056, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065db98cdb764f7787a33b548628dd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the optimizer\n",
    "epochs = 10\n",
    "optimizer = optim.SGD(clf.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for e in tqdm(range(epochs)):\n",
    "    for data, label in dataloader:\n",
    "        out = clf(data)\n",
    "        loss = criterion(out, label.squeeze(1))\n",
    "        optimizer.zero_grad\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_points = []\n",
    "test_class_labels = []\n",
    "\n",
    "with open('mnist_test.csv') as fs:\n",
    "    for line in fs:\n",
    "        data = list(map(int, line.strip().split(','))) \n",
    "        label = data[0]\n",
    "        datapoint = data[1:]\n",
    "        test_data_points.append(datapoint)\n",
    "        test_class_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mnist_data = Mnist_dataset(test_data_points, test_class_labels)\n",
    "testloader = DataLoader(test_mnist_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for data, label in testloader:\n",
    "        out = clf(data)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        ground_truth.extend(label.reshape(-1).numpy().tolist())\n",
    "        predictions.extend(pred.reshape(-1).numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8098"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('i2h.weight',\n",
       "              tensor([[ 0.0257,  0.0392, -0.0265,  ...,  0.0122,  0.0208,  0.0304],\n",
       "                      [-0.0097,  0.0094, -0.0509,  ...,  0.0029,  0.0010,  0.0422],\n",
       "                      [ 0.0114,  0.0229, -0.0210,  ...,  0.0053,  0.0013,  0.0514],\n",
       "                      ...,\n",
       "                      [-0.0017, -0.0256, -0.0152,  ...,  0.0431,  0.0117, -0.0037],\n",
       "                      [ 0.0320,  0.0567, -0.0085,  ..., -0.0027,  0.0028, -0.0359],\n",
       "                      [ 0.0106, -0.0102, -0.0372,  ...,  0.0429,  0.0526,  0.0190]])),\n",
       "             ('i2h.bias',\n",
       "              tensor([ 0.0346, -0.1071,  0.0148,  ..., -0.0476, -0.0288, -0.0131])),\n",
       "             ('norm.weight',\n",
       "              tensor([  1.1315, 123.8293,  -9.2225,  ...,   0.3094,  -6.8292,  18.3751])),\n",
       "             ('norm.bias',\n",
       "              tensor([-14.1297, -26.4977, -21.6721,  ...,  -0.9982, -16.9196,  -5.4343])),\n",
       "             ('h2o.weight',\n",
       "              tensor([[ 7.9275e+00,  1.0292e+01,  1.4945e+00,  ..., -2.9980e+00,\n",
       "                       -1.5006e+01,  6.5193e+00],\n",
       "                      [-1.9035e+00, -1.1312e+00, -2.1273e-01,  ...,  1.1409e+01,\n",
       "                        2.2407e+01, -8.2744e+00],\n",
       "                      [ 5.6570e+00, -4.7704e+00,  4.2393e-01,  ..., -1.6357e+00,\n",
       "                       -8.1917e+00,  1.3502e-02],\n",
       "                      ...,\n",
       "                      [-8.9510e+00, -8.4251e+00, -5.3272e+00,  ...,  4.9533e+00,\n",
       "                        1.2405e+01, -1.2917e+01],\n",
       "                      [-6.7480e+00,  4.9354e+00,  9.2330e+00,  ..., -2.8720e+00,\n",
       "                       -9.9885e+00,  7.3427e+00],\n",
       "                      [-6.2831e+00, -1.0750e+00,  2.2555e+00,  ..., -3.1809e+00,\n",
       "                        3.6531e+00,  1.3837e+01]])),\n",
       "             ('h2o.bias',\n",
       "              tensor([-1.0914,  0.2890, -1.6839,  2.3399, -1.4754,  0.6732, -6.8067,  1.1650,\n",
       "                       4.4507,  2.1182]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(clf.state_dict(), 'model_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier_1(784, 1056, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_state_dict(torch.load('model_mnist.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the task of Human activity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Download the dataset from https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each datapoints consists of smartphone sensor measurements and your task is to predict the activity the person is performing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 561 features per data point and a total of 6 activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and text split are already available.. ~/train/X_train, ~/train/y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561\n"
     ]
    }
   ],
   "source": [
    "with open('human_activity_recognition/train/X_train.txt') as fs:\n",
    "    for line in fs:\n",
    "        features = line.strip().split()\n",
    "        print(len(features))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Write Torch dataset class for the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAR_dataset(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Create a neural network architecture for the task. (play around a bit with number of layers/number of nodes perlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Write a train function to train the model on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model at the end of each epoch\n",
    "def train(clf, train_data, batch_size, epochs, learning_rate):\n",
    "    '''\n",
    "    clf: classifier model\n",
    "    train_data: pytorch dataset\n",
    "    '''\n",
    "    # define the optimizer \n",
    "    # define the loss criteria\n",
    "    for _ in tqdm(range(epochs)): # the models are trained over multiple epochs..\n",
    "        train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "        for d, l in train_dataloader:\n",
    "            ...\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Write a test function which loads a model and computes the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, test_data):\n",
    "    '''\n",
    "    model_path: Path to the saved model\n",
    "    test_data: pytorch dataset\n",
    "    '''\n",
    "    # clf: load model from model_path\n",
    "    test_dataloader = DataLoader(test_data, batch_size=100) # evaluate over a batch of examples which reduces time\n",
    "    clf.eval()\n",
    "    for d, l in test_dataloader:\n",
    "        ...\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting everything together\n",
    "clf = Classifier()\n",
    "train_data = HAR_dataset()\n",
    "test_data = HAR_dataset()\n",
    "batch_size = \n",
    "epochs = \n",
    "train(clf, train_data, batch_size, epochs)\n",
    "evaluate(model_path, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
