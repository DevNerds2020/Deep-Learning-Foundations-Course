1. Optimizers: Momentum, Learning Rate, and Learning Rate Decay

Momentum: It helps accelerate optimization by smoothing updates using information from past gradients, which reduces oscillations and speeds up convergence.

Learning Rate: It determines the size of updates to the modelâ€™s weights during training. A proper learning rate balances fast convergence and stability.

Learning Rate Decay: This gradually reduces the learning rate as training progresses to allow finer adjustments and prevent overshooting the optimal solution.

2. Dropout Regularization

Dropout randomly disables neurons during training, forcing the network to learn more robust and generalized patterns, which helps prevent overfitting.

3. Convolutions

For a 10x10 image and a stride of 2, a 2x2 filter size is needed to produce a 5x5 feature map.
